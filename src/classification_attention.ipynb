{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc57710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create batch data sets from raw data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=1024\n",
    ").batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val, y_val)\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "print(\"train_dataset.element_spec: {}\".format(train_dataset.element_spec))\n",
    "print(\"val_dataset.element_spec: {}\".format(val_dataset.element_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define loss and optimizer for training.\"\"\"\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,  # Compute loss from softmax, not from logits.\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c72281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define metrics for training and validation.\"\"\"\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean(\n",
    "    name='training_loss',\n",
    ")\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy(\n",
    "    name=\"training_accuracy\",\n",
    ")\n",
    "\n",
    "val_loss_metric = tf.keras.metrics.Mean(\n",
    "    name='validation_loss',\n",
    ")\n",
    "\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy(\n",
    "    name=\"validation_accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558552b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define MultiHeadAttention layer.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k == seq_len_v.\n",
    "    \n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (batch_size, num_heads, (seq_len_q,) depth)\n",
    "      k: key shape   == (batch_size, num_heads, (seq_len_k,) depth)\n",
    "      v: value shape == (batch_size, num_heads, (seq_len_k,) depth)\n",
    "      mask: Float tensor with shape broadcastable\n",
    "            to (batch_size, num_heads, seq_len_q, seq_len_k) (default: None)\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"BEGIN Attention\")\n",
    "    #print(\"q.shape == {}\".format(q.shape))\n",
    "    #print(\"k.shape == {}\".format(k.shape))\n",
    "    #print(\"v.shape == {}\".format(v.shape))\n",
    "\n",
    "    \"\"\"Matrix multiplication of last two dimensions of query and key,\n",
    "    if necessary after reshaping with dummy dimension to emulate outer product of two vectors\n",
    "    \"\"\"\n",
    "    if len(q.shape) == 3:\n",
    "        attention_logits = tf.matmul(\n",
    "            tf.transpose(\n",
    "                tf.reshape(\n",
    "                    q, \n",
    "                    (\n",
    "                        tf.shape(q)[0],  # batch size\n",
    "                        tf.shape(q)[1],  # num_heads\n",
    "                        1,               # dummy dimension for sequence of length=1\n",
    "                        tf.shape(q)[2],  # depth\n",
    "                    )\n",
    "                ),\n",
    "                perm=[0,1,3,2]  # transpose vector in penultima and ultima dimension for multiplication\n",
    "            ),\n",
    "            tf.reshape(\n",
    "                k, \n",
    "                (\n",
    "                    tf.shape(k)[0],  # batch size\n",
    "                    tf.shape(k)[1],  # num_heads\n",
    "                    1,               # dummy dimension for sequence of length=1\n",
    "                    tf.shape(k)[2],  # depth\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    elif len(x.shape) == 4:\n",
    "        attention_logits = tf.matmul(\n",
    "            q,\n",
    "            k, \n",
    "            transpose_b=True\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #print(\"attention_logits.shape == {}\".format(attention_logits.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    \"\"\"Scale matmul_qk element-wise by depth.\"\"\"\n",
    "    scaled_attention_logits = attention_logits / tf.math.sqrt(\n",
    "        tf.cast(\n",
    "            tf.shape(k)[-1], \n",
    "            tf.float32\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \"\"\"Add mask to scaled tensor.\"\"\"\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    \"\"\"Softmax the last axis (seq_len_k).\"\"\"\n",
    "    attention_weights = tf.nn.softmax(\n",
    "        scaled_attention_logits, \n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    #print(\"attention_weights.shape == {}\".format(attention_weights.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    \"\"\"Scale value with attention weights.\"\"\"\n",
    "    if len(v.shape) == 3:\n",
    "        output = tf.matmul(\n",
    "            attention_weights, \n",
    "            tf.transpose(\n",
    "                tf.reshape(\n",
    "                    v,\n",
    "                    (\n",
    "                        tf.shape(v)[0],  # batch size\n",
    "                        tf.shape(v)[1],  # num_heads\n",
    "                        1,               # dummy dimension for sequence of length=1\n",
    "                        tf.shape(v)[2],  # depth\n",
    "                    )\n",
    "                ),\n",
    "                perm=[0,1,3,2]  # transpose vector in penultima and ultima dimension for multiplication\n",
    "            )\n",
    "        )\n",
    "    elif len(v.shape) == 4:\n",
    "        output = tf.matmul(\n",
    "            attention_weights, \n",
    "            v\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    #print(\"output.shape == {}\".format(output.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "    \n",
    "    #print(\"END Attention\")\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.dense_q = tf.keras.layers.Dense(d_model)\n",
    "        self.dense_k = tf.keras.layers.Dense(d_model)\n",
    "        self.dense_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size, do_transpose=False):\n",
    "        \"\"\"\n",
    "        Split last dimension into num_heads times depth\n",
    "        and ensure that num_heads is 2nd dimension.\n",
    "        \"\"\"      \n",
    "        if do_transpose:\n",
    "            \"\"\"Shape length of 3 indicates an additional dimension for sequence.\n",
    "            \n",
    "            (batch_size, seq_length, d_model)\n",
    "            ->\n",
    "            (batch_size, seq_length, num_heads, depth)\n",
    "            ->\n",
    "            (batch_size, num_heads, seq_length, depth)\n",
    "            \"\"\"\n",
    "            return tf.transpose(\n",
    "                tf.reshape(\n",
    "                    x, \n",
    "                    (\n",
    "                        batch_size, \n",
    "                        -1,\n",
    "                        self.num_heads, \n",
    "                        self.depth,\n",
    "                    )\n",
    "                ), \n",
    "                perm=[0, 2, 1, 3]\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            \"\"\"\n",
    "            (batch_size, d_model)\n",
    "            ->\n",
    "            (batch_size, num_heads, depth)\n",
    "            \"\"\"\n",
    "            return tf.reshape(\n",
    "                x, \n",
    "                (\n",
    "                    batch_size, \n",
    "                    self.num_heads, \n",
    "                    self.depth,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]  # (batch_size, ..., x_dim)\n",
    "        \n",
    "        q = x  # Query\n",
    "        k = x  # Key\n",
    "        v = x  # Value\n",
    "        \n",
    "        #print(\"q.shape == {}\".format(q.shape))\n",
    "\n",
    "        dense_q = self.dense_q(q)  # (batch_size, (seq_len,) d_model)\n",
    "        dense_k = self.dense_k(k)  # (batch_size, (seq_len,) d_model)\n",
    "        dense_v = self.dense_v(v)  # (batch_size, (seq_len,) d_model)\n",
    "        \n",
    "        #print(\"dense_q.shape == {}\".format(dense_q.shape))\n",
    "        \n",
    "        \"\"\"Shape length of 3 indicates an additional dimension for sequence \n",
    "        which requires transposition in split.\n",
    "        \"\"\"\n",
    "        do_transpose = len(dense_q.shape) == 3\n",
    "\n",
    "        split_q = self.split_heads(dense_q, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        split_k = self.split_heads(dense_k, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        split_v = self.split_heads(dense_v, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        \n",
    "        #print(\"split_q.shape == {}\".format(split_q.shape))\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, (seq_len,) depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, )\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            split_q, \n",
    "            split_k, \n",
    "            split_v, \n",
    "            mask=None\n",
    "        )\n",
    "        \n",
    "        #print(\"scaled_attention.shape == {}\".format(scaled_attention.shape))\n",
    "        #print(\"attention_weights.shape == {}\".format(attention_weights.shape))\n",
    "\n",
    "        if do_transpose:\n",
    "            \"\"\"Revert transposition from head split.\n",
    "            \n",
    "            (batch_size, num_heads, seq_len, depth)\n",
    "            ->\n",
    "            (batch_size, seq_length, num_heads, depth)\n",
    "            \"\"\"\n",
    "            scaled_attention = tf.transpose(\n",
    "                scaled_attention,\n",
    "                perm=[0, 2, 1, 3]\n",
    "            ) \n",
    "            #print(\"transposed scaled_attention.shape == {}\".format(scaled_attention.shape))\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (\n",
    "                -1, \n",
    "                self.d_model\n",
    "            )\n",
    "        )  # (batch_size, d_model)\n",
    "        \n",
    "        #print(\"concat_attention.shape == {}\".format(concat_attention.shape))\n",
    "\n",
    "        output = self.dense(\n",
    "            concat_attention\n",
    "        )  # (batch_size, d_model)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model.\"\"\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "class OverlyComplicatedModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = keras.Input(\n",
    "            shape=(input_dim,),\n",
    "        )\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            48,\n",
    "            num_heads=4,\n",
    "        )\n",
    "        \n",
    "        self.normalization_1 = keras.layers.BatchNormalization(\n",
    "            epsilon=1e-6,\n",
    "        )\n",
    "        \n",
    "        self.dense = keras.layers.Dense(\n",
    "            48,\n",
    "            activation=\"relu\",\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(\n",
    "            0.5,\n",
    "        )\n",
    "        \n",
    "        self.normalization_2 = keras.layers.BatchNormalization(\n",
    "            epsilon=1e-6,\n",
    "        )\n",
    "        \n",
    "        self.outputs = keras.layers.Dense(\n",
    "            output_dim,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        #x = self.input_layer(inputs)\n",
    "        \n",
    "        x_attn, attention_weights = self.multi_head_attention(inputs)\n",
    "        \n",
    "        x_attn_res = keras.layers.add(\n",
    "            [\n",
    "                inputs,  # Residual connection\n",
    "                x_attn,  \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        x_attn_norm = self.normalization_1(x_attn_res)\n",
    "        \n",
    "        #print(\"x_attn shape: {}\".format(x_attn.shape))\n",
    "        #print(\"Inputs shape: {}\".format(inputs.shape))\n",
    "        \n",
    "        x_dense = self.dense(x_attn_norm)\n",
    "        \n",
    "        x_dense_res = keras.layers.add(\n",
    "            [\n",
    "                x_attn_norm,  # Residual connection\n",
    "                x_dense,  \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #x = self.dropout(\n",
    "        #    x_dense_res, \n",
    "        #    training=training,\n",
    "        #)\n",
    "        \n",
    "        x = self.normalization_2(x_dense_res)\n",
    "        \n",
    "        outputs = self.outputs(x)\n",
    "        \n",
    "        return outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create model.\"\"\"\n",
    "\n",
    "model = OverlyComplicatedModel(\n",
    "    input_dim=x_ar.shape[1],\n",
    "    output_dim=y_ar.shape[1],\n",
    ")\n",
    "\n",
    "\"\"\"The model has NOT yet been built.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the step function as a callable TensorFlow graph.\"\"\"\n",
    "\n",
    "@tf.function()\n",
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"Record operations for automatic differentiation.\"\"\"\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        #print(x_batch.shape)\n",
    "        #print(y_batch.shape)\n",
    "        \n",
    "        predictions, _ = model(x_batch, training=True) \n",
    "\n",
    "        loss = loss_fn(\n",
    "            y_batch, \n",
    "            predictions,\n",
    "        )\n",
    "        \n",
    "    \"\"\"Compute gradients.\"\"\"\n",
    "    gradients = gradient_tape.gradient(loss, model.trainable_weights)\n",
    "        \n",
    "    \"\"\"Compute and apply deltas.\"\"\"\n",
    "    optimizer.apply_gradients(\n",
    "        zip(\n",
    "            gradients, \n",
    "            model.trainable_weights,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_loss_metric(loss)\n",
    "    train_acc_metric(y_batch, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model.\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = defaultdict(list)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss_metric.reset_states()\n",
    "    train_acc_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    val_acc_metric.reset_states()\n",
    "\n",
    "    \"\"\"Training\"\"\"\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Epoch %02d | step %04d | %06s samples | loss: %.4f\" % (\n",
    "                    epoch,\n",
    "                    step,\n",
    "                    step * batch_size,\n",
    "                    float(loss)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    history[\"loss\"].append(train_loss_metric.result())\n",
    "    history[\"accuracy\"].append(train_acc_metric.result())\n",
    "\n",
    "    print(\n",
    "        \"Epoch %02d | training accuracy: %.4f\" % (\n",
    "            epoch,\n",
    "            float(train_acc_metric.result())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \"\"\"Validation\"\"\"\n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        predictions, attention_weights = model(x_batch, training=False)\n",
    "        \n",
    "        loss = loss_fn(y_batch, predictions)\n",
    "        \n",
    "        val_loss_metric.update_state(loss)\n",
    "        val_acc_metric.update_state(y_batch, predictions)\n",
    "        \n",
    "        #print(attention_weights.shape)\n",
    "        \n",
    "    history[\"val_loss\"].append(val_loss_metric.result())\n",
    "    history[\"val_accuracy\"].append(val_acc_metric.result())\n",
    "    history[\"attention_weights\"].append(attention_weights)\n",
    "\n",
    "    print(\n",
    "        \"Epoch %02d | validation accuracy: %.4f\" % (\n",
    "            epoch,\n",
    "            float(val_acc_metric.result())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        \"Epoch %02d | time: %.2fs\\n\" % (\n",
    "            epoch,\n",
    "            time.time() - start_time\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f660f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracies.\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    \n",
    "    plt.title('model accuracy')\n",
    "    \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    \n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    plt.savefig('accuracy')    \n",
    "    \n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot losses.\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses(history):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    \n",
    "    plt.title('model loss')\n",
    "    \n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    \n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    plt.savefig('loss')\n",
    "    \n",
    "plot_losses(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326bc7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0             1         2         3             4         5   \\\n",
      "0     -3.014600e-07  8.260300e-06 -0.000012 -0.000002 -1.438600e-06 -0.000021   \n",
      "1      2.913200e-06 -5.247700e-06  0.000003 -0.000006  2.778900e-06 -0.000004   \n",
      "2     -2.951700e-06 -3.184000e-06 -0.000016 -0.000001 -1.575300e-06  0.000017   \n",
      "3     -1.322600e-06  8.820100e-06 -0.000016 -0.000005 -7.282900e-07  0.000004   \n",
      "4     -6.836600e-08  5.666300e-07 -0.000026 -0.000006 -7.940600e-07  0.000013   \n",
      "...             ...           ...       ...       ...           ...       ...   \n",
      "58504 -9.982500e-06  1.854900e-06 -0.000021  0.000021 -5.910200e-06 -0.000098   \n",
      "58505 -1.055400e-05  1.983500e-05  0.000031 -0.000018 -1.061600e-04  0.000292   \n",
      "58506 -5.857700e-06  1.859400e-05 -0.000102 -0.000003  3.827500e-06  0.000117   \n",
      "58507 -4.441100e-06  3.396900e-05 -0.000442  0.000005  6.500800e-06  0.000087   \n",
      "58508 -8.853300e-06  5.225900e-05  0.000072  0.000010  3.795600e-06 -0.000032   \n",
      "\n",
      "             6         7         8         9   ...       39       40       41  \\\n",
      "0      0.031718  0.031710  0.031721 -0.032963  ... -0.63308   2.9646   8.1198   \n",
      "1      0.030804  0.030810  0.030806 -0.033520  ... -0.59314   7.6252   6.1690   \n",
      "2      0.032877  0.032880  0.032896 -0.029834  ... -0.63252   2.7784   5.3017   \n",
      "3      0.029410  0.029401  0.029417 -0.030156  ... -0.62289   6.5534   6.2606   \n",
      "4      0.030119  0.030119  0.030145 -0.031393  ... -0.63010   4.5155   9.5231   \n",
      "...         ...       ...       ...       ...  ...      ...      ...      ...   \n",
      "58504 -0.083417 -0.083419 -0.083398 -0.182340  ... -0.52907   1.4641   7.0032   \n",
      "58505 -0.085131 -0.085151 -0.085182 -0.184320  ... -0.51971   3.3275   2.3072   \n",
      "58506 -0.081989 -0.082008 -0.081906 -0.186140  ... -0.51103  20.9250   9.0437   \n",
      "58507 -0.081500 -0.081534 -0.081093 -0.183630  ... -0.52033   1.3890  10.7430   \n",
      "58508 -0.083034 -0.083086 -0.083159 -0.185890  ... -0.50974   1.6026   4.5773   \n",
      "\n",
      "           42      43      44      45      46      47  48  \n",
      "0     -1.4961 -1.4961 -1.4961 -1.4996 -1.4996 -1.4996   1  \n",
      "1     -1.4967 -1.4967 -1.4967 -1.5005 -1.5005 -1.5005   1  \n",
      "2     -1.4983 -1.4983 -1.4982 -1.4985 -1.4985 -1.4985   1  \n",
      "3     -1.4963 -1.4963 -1.4963 -1.4975 -1.4975 -1.4976   1  \n",
      "4     -1.4958 -1.4958 -1.4958 -1.4959 -1.4959 -1.4959   1  \n",
      "...       ...     ...     ...     ...     ...     ...  ..  \n",
      "58504 -1.5024 -1.5025 -1.5023 -1.4933 -1.4933 -1.4933  11  \n",
      "58505 -1.5024 -1.5025 -1.5024 -1.4925 -1.4925 -1.4926  11  \n",
      "58506 -1.5035 -1.5035 -1.5039 -1.4911 -1.4912 -1.4910  11  \n",
      "58507 -1.5029 -1.5029 -1.5030 -1.4932 -1.4932 -1.4931  11  \n",
      "58508 -1.5039 -1.5040 -1.5036 -1.4945 -1.4946 -1.4943  11  \n",
      "\n",
      "[58509 rows x 49 columns]\n",
      "(58509, 49)\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "30    0\n",
      "31    0\n",
      "32    0\n",
      "33    0\n",
      "34    0\n",
      "35    0\n",
      "36    0\n",
      "37    0\n",
      "38    0\n",
      "39    0\n",
      "40    0\n",
      "41    0\n",
      "42    0\n",
      "43    0\n",
      "44    0\n",
      "45    0\n",
      "46    0\n",
      "47    0\n",
      "48    0\n",
      "dtype: int64\n",
      "x type: <class 'pandas.core.frame.DataFrame'>\n",
      "x shape: (58509, 48)\n",
      "y type: <class 'pandas.core.series.Series'>\n",
      "y shape: (58509,)\n",
      "                 0             1             2             3             4   \\\n",
      "count  58509.000000  5.850900e+04  5.850900e+04  58509.000000  5.850900e+04   \n",
      "mean      -0.000003  1.439648e-06  1.412013e-06     -0.000001  1.351239e-06   \n",
      "std        0.000072  5.555429e-05  2.353009e-04      0.000063  5.660943e-05   \n",
      "min       -0.013721 -5.414400e-03 -1.358000e-02     -0.012787 -8.355900e-03   \n",
      "25%       -0.000007 -1.444400e-05 -7.239600e-05     -0.000005 -1.475300e-05   \n",
      "50%       -0.000003  8.804600e-07  5.137700e-07     -0.000001  7.540200e-07   \n",
      "75%        0.000002  1.877700e-05  7.520000e-05      0.000004  1.906200e-05   \n",
      "max        0.005784  4.525300e-03  5.237700e-03      0.001453  8.245100e-04   \n",
      "\n",
      "                 5             6             7             8             9   \\\n",
      "count  5.850900e+04  58509.000000  58509.000000  58509.000000  58509.000000   \n",
      "mean  -2.654483e-07      0.001915      0.001913      0.001912     -0.011897   \n",
      "std    2.261907e-04      0.036468      0.036465      0.036470      0.066482   \n",
      "min   -9.741300e-03     -0.139890     -0.135940     -0.130860     -0.218640   \n",
      "25%   -7.379100e-05     -0.019927     -0.019951     -0.019925     -0.032144   \n",
      "50%   -1.659300e-07      0.013226      0.013230      0.013247     -0.015566   \n",
      "75%    7.138600e-05      0.024770      0.024776      0.024777      0.020614   \n",
      "max    2.753600e-03      0.069125      0.069130      0.069131      0.352580   \n",
      "\n",
      "       ...            38            39            40            41  \\\n",
      "count  ...  58509.000000  58509.000000  58509.000000  58509.000000   \n",
      "mean   ...      8.406765     -0.397757      7.293781      8.273772   \n",
      "std    ...      6.897301     25.018728     12.451781      6.565952   \n",
      "min    ...      0.522180     -0.902350     -0.596830      0.320660   \n",
      "25%    ...      4.451300     -0.715470      1.450300      4.436300   \n",
      "50%    ...      6.566800     -0.661710      3.301300      6.479100   \n",
      "75%    ...      9.952600     -0.573980      8.288500      9.857500   \n",
      "max    ...    265.330000   3670.800000    889.930000    153.150000   \n",
      "\n",
      "                 42            43            44            45            46  \\\n",
      "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
      "mean      -1.500887     -1.500912     -1.500805     -1.497771     -1.497794   \n",
      "std        0.003657      0.003668      0.003632      0.003163      0.003163   \n",
      "min       -1.525500     -1.526200     -1.523700     -1.521400     -1.523200   \n",
      "25%       -1.503300     -1.503400     -1.503200     -1.499600     -1.499600   \n",
      "50%       -1.500300     -1.500300     -1.500300     -1.498100     -1.498100   \n",
      "75%       -1.498200     -1.498200     -1.498200     -1.496200     -1.496300   \n",
      "max       -1.457600     -1.456100     -1.455500     -1.337200     -1.337200   \n",
      "\n",
      "                 47  \n",
      "count  58509.000000  \n",
      "mean      -1.497686  \n",
      "std        0.003175  \n",
      "min       -1.521300  \n",
      "25%       -1.499500  \n",
      "50%       -1.498000  \n",
      "75%       -1.496200  \n",
      "max       -1.337100  \n",
      "\n",
      "[8 rows x 48 columns]\n",
      "48\n",
      "1     5319\n",
      "2     5319\n",
      "3     5319\n",
      "4     5319\n",
      "5     5319\n",
      "6     5319\n",
      "7     5319\n",
      "8     5319\n",
      "9     5319\n",
      "10    5319\n",
      "11    5319\n",
      "dtype: int64\n",
      "             0         1         2         3         4         5         6   \\\n",
      "0      0.703460  0.545556  0.721049  0.897795  0.910031  0.777923  0.821032   \n",
      "1      0.703624  0.544197  0.721839  0.897532  0.910491  0.779322  0.816659   \n",
      "2      0.703324  0.544404  0.720815  0.897872  0.910017  0.781014  0.826577   \n",
      "3      0.703407  0.545612  0.720817  0.897619  0.910109  0.779954  0.809990   \n",
      "4      0.703472  0.544782  0.720284  0.897501  0.910102  0.780702  0.813382   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "58504  0.702963  0.544911  0.720542  0.899397  0.909544  0.771796  0.270186   \n",
      "58505  0.702934  0.546720  0.723325  0.896692  0.898624  0.802974  0.261986   \n",
      "58506  0.703175  0.546595  0.716256  0.897713  0.910605  0.789020  0.277018   \n",
      "58507  0.703247  0.548142  0.698191  0.898320  0.910896  0.786611  0.279358   \n",
      "58508  0.703021  0.549982  0.725512  0.898657  0.910602  0.777050  0.272019   \n",
      "\n",
      "             7         8         9   ...        38        39        40  \\\n",
      "0      0.817526  0.812942  0.325053  ...  0.020749  0.000073  0.003999   \n",
      "1      0.813137  0.808366  0.324078  ...  0.011641  0.000084  0.009233   \n",
      "2      0.823231  0.818817  0.330531  ...  0.019933  0.000073  0.003790   \n",
      "3      0.806266  0.801421  0.329967  ...  0.086379  0.000076  0.008029   \n",
      "4      0.809767  0.805061  0.327802  ...  0.017129  0.000074  0.005741   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "58504  0.256113  0.237321  0.063548  ...  0.025950  0.000102  0.002314   \n",
      "58505  0.247667  0.228400  0.060082  ...  0.012586  0.000104  0.004407   \n",
      "58506  0.262993  0.244781  0.056896  ...  0.045965  0.000107  0.024168   \n",
      "58507  0.265305  0.248846  0.061290  ...  0.014615  0.000104  0.002230   \n",
      "58508  0.257736  0.238516  0.057333  ...  0.019369  0.000107  0.002470   \n",
      "\n",
      "             41        42        43        44        45        46        47  \n",
      "0      0.051032  0.432990  0.429387  0.404692  0.118350  0.126882  0.117807  \n",
      "1      0.038267  0.424153  0.420827  0.395894  0.113464  0.122043  0.112921  \n",
      "2      0.032592  0.400589  0.398003  0.373900  0.124321  0.132796  0.123779  \n",
      "3      0.038866  0.430044  0.426534  0.401760  0.129750  0.138172  0.128664  \n",
      "4      0.060214  0.437408  0.433666  0.409091  0.138436  0.146774  0.137894  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "58504  0.043726  0.340206  0.338088  0.313783  0.152552  0.160753  0.152009  \n",
      "58505  0.012998  0.340206  0.338088  0.312317  0.156895  0.165054  0.155809  \n",
      "58506  0.057077  0.324006  0.323823  0.290323  0.164495  0.172043  0.164495  \n",
      "58507  0.068196  0.332842  0.332382  0.303519  0.153094  0.161290  0.153094  \n",
      "58508  0.027852  0.318115  0.316690  0.294721  0.146037  0.153763  0.146580  \n",
      "\n",
      "[58509 rows x 48 columns]\n",
      "x_df type: <class 'pandas.core.frame.DataFrame'>\n",
      "x_df shape: (58509, 48)\n",
      "       1   2   3   4   5   6   7   8   9   10  11\n",
      "0       1   0   0   0   0   0   0   0   0   0   0\n",
      "1       1   0   0   0   0   0   0   0   0   0   0\n",
      "2       1   0   0   0   0   0   0   0   0   0   0\n",
      "3       1   0   0   0   0   0   0   0   0   0   0\n",
      "4       1   0   0   0   0   0   0   0   0   0   0\n",
      "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
      "58504   0   0   0   0   0   0   0   0   0   0   1\n",
      "58505   0   0   0   0   0   0   0   0   0   0   1\n",
      "58506   0   0   0   0   0   0   0   0   0   0   1\n",
      "58507   0   0   0   0   0   0   0   0   0   0   1\n",
      "58508   0   0   0   0   0   0   0   0   0   0   1\n",
      "\n",
      "[58509 rows x 11 columns]\n",
      "y_df type: <class 'pandas.core.frame.DataFrame'>\n",
      "y_df shape: (58509, 11)\n",
      "x_ar type: <class 'numpy.ndarray'>\n",
      "x_ar shape: (58509, 48)\n",
      "y_ar type: <class 'numpy.ndarray'>\n",
      "y_ar shape: (58509, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ar shape: (58509, 48)\n",
      "x_val shape: (11702, 48)\n",
      "x_test shape: (11702, 48)\n",
      "x_train shape: (35105, 48)\n",
      "y_ar shape: (58509, 11)\n",
      "y_val shape: (11702, 11)\n",
      "y_test shape: (11702, 11)\n",
      "y_train shape: (35105, 11)\n"
     ]
    }
   ],
   "source": [
    "%run ./data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc57710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.element_spec: (TensorSpec(shape=(None, 48), dtype=tf.float64, name=None), TensorSpec(shape=(None, 11), dtype=tf.uint8, name=None))\n",
      "val_dataset.element_spec: (TensorSpec(shape=(None, 48), dtype=tf.float64, name=None), TensorSpec(shape=(None, 11), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create batch data sets from raw data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=1024\n",
    ").batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val, y_val)\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "print(\"train_dataset.element_spec: {}\".format(train_dataset.element_spec))\n",
    "print(\"val_dataset.element_spec: {}\".format(val_dataset.element_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define loss and optimizer for training.\"\"\"\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,  # Compute loss from softmax, not from logits.\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c72281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define metrics for training and validation.\"\"\"\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean(\n",
    "    name='training_loss',\n",
    ")\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy(\n",
    "    name=\"training_accuracy\",\n",
    ")\n",
    "\n",
    "val_loss_metric = tf.keras.metrics.Mean(\n",
    "    name='validation_loss',\n",
    ")\n",
    "\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy(\n",
    "    name=\"validation_accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558552b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define MultiHeadAttention layer.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k == seq_len_v.\n",
    "    \n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (batch_size, num_heads, (seq_len_q,) depth)\n",
    "      k: key shape   == (batch_size, num_heads, (seq_len_k,) depth)\n",
    "      v: value shape == (batch_size, num_heads, (seq_len_k,) depth)\n",
    "      mask: Float tensor with shape broadcastable\n",
    "            to (batch_size, num_heads, seq_len_q, seq_len_k) (default: None)\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"BEGIN Attention\")\n",
    "    #print(\"q.shape == {}\".format(q.shape))\n",
    "    #print(\"k.shape == {}\".format(k.shape))\n",
    "    #print(\"v.shape == {}\".format(v.shape))\n",
    "\n",
    "    \"\"\"Matrix multiplication of last two dimensions of query and key,\n",
    "    if necessary after reshaping with dummy dimension to emulate outer product of two vectors\n",
    "    \"\"\"\n",
    "    if len(q.shape) == 3:\n",
    "        attention_logits = tf.matmul(\n",
    "            tf.transpose(\n",
    "                tf.reshape(\n",
    "                    q, \n",
    "                    (\n",
    "                        tf.shape(q)[0],  # batch size\n",
    "                        tf.shape(q)[1],  # num_heads\n",
    "                        1,               # dummy dimension for sequence of length=1\n",
    "                        tf.shape(q)[2],  # depth\n",
    "                    )\n",
    "                ),\n",
    "                perm=[0,1,3,2]  # transpose vector in penultima and ultima dimension for multiplication\n",
    "            ),\n",
    "            tf.reshape(\n",
    "                k, \n",
    "                (\n",
    "                    tf.shape(k)[0],  # batch size\n",
    "                    tf.shape(k)[1],  # num_heads\n",
    "                    1,               # dummy dimension for sequence of length=1\n",
    "                    tf.shape(k)[2],  # depth\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    elif len(x.shape) == 4:\n",
    "        attention_logits = tf.matmul(\n",
    "            q,\n",
    "            k, \n",
    "            transpose_b=True\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #print(\"attention_logits.shape == {}\".format(attention_logits.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    \"\"\"Scale matmul_qk element-wise by depth.\"\"\"\n",
    "    scaled_attention_logits = attention_logits / tf.math.sqrt(\n",
    "        tf.cast(\n",
    "            tf.shape(k)[-1], \n",
    "            tf.float32\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \"\"\"Add mask to scaled tensor.\"\"\"\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    \"\"\"Softmax the last axis (seq_len_k).\"\"\"\n",
    "    attention_weights = tf.nn.softmax(\n",
    "        scaled_attention_logits, \n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    #print(\"attention_weights.shape == {}\".format(attention_weights.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    \"\"\"Scale value with attention weights.\"\"\"\n",
    "    if len(v.shape) == 3:\n",
    "        output = tf.matmul(\n",
    "            attention_weights, \n",
    "            tf.transpose(\n",
    "                tf.reshape(\n",
    "                    v,\n",
    "                    (\n",
    "                        tf.shape(v)[0],  # batch size\n",
    "                        tf.shape(v)[1],  # num_heads\n",
    "                        1,               # dummy dimension for sequence of length=1\n",
    "                        tf.shape(v)[2],  # depth\n",
    "                    )\n",
    "                ),\n",
    "                perm=[0,1,3,2]  # transpose vector in penultima and ultima dimension for multiplication\n",
    "            )\n",
    "        )\n",
    "    elif len(v.shape) == 4:\n",
    "        output = tf.matmul(\n",
    "            attention_weights, \n",
    "            v\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    #print(\"output.shape == {}\".format(output.shape))\n",
    "    # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "    \n",
    "    #print(\"END Attention\")\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.dense_q = tf.keras.layers.Dense(d_model)\n",
    "        self.dense_k = tf.keras.layers.Dense(d_model)\n",
    "        self.dense_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size, do_transpose=False):\n",
    "        \"\"\"\n",
    "        Split last dimension into num_heads times depth\n",
    "        and ensure that num_heads is 2nd dimension.\n",
    "        \"\"\"      \n",
    "        if do_transpose:\n",
    "            \"\"\"Shape length of 3 indicates an additional dimension for sequence.\n",
    "            \n",
    "            (batch_size, seq_length, d_model)\n",
    "            ->\n",
    "            (batch_size, seq_length, num_heads, depth)\n",
    "            ->\n",
    "            (batch_size, num_heads, seq_length, depth)\n",
    "            \"\"\"\n",
    "            return tf.transpose(\n",
    "                tf.reshape(\n",
    "                    x, \n",
    "                    (\n",
    "                        batch_size, \n",
    "                        -1,\n",
    "                        self.num_heads, \n",
    "                        self.depth,\n",
    "                    )\n",
    "                ), \n",
    "                perm=[0, 2, 1, 3]\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            \"\"\"\n",
    "            (batch_size, d_model)\n",
    "            ->\n",
    "            (batch_size, num_heads, depth)\n",
    "            \"\"\"\n",
    "            return tf.reshape(\n",
    "                x, \n",
    "                (\n",
    "                    batch_size, \n",
    "                    self.num_heads, \n",
    "                    self.depth,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]  # (batch_size, ..., x_dim)\n",
    "        \n",
    "        q = x  # Query\n",
    "        k = x  # Key\n",
    "        v = x  # Value\n",
    "        \n",
    "        #print(\"q.shape == {}\".format(q.shape))\n",
    "\n",
    "        dense_q = self.dense_q(q)  # (batch_size, (seq_len,) d_model)\n",
    "        dense_k = self.dense_k(k)  # (batch_size, (seq_len,) d_model)\n",
    "        dense_v = self.dense_v(v)  # (batch_size, (seq_len,) d_model)\n",
    "        \n",
    "        #print(\"dense_q.shape == {}\".format(dense_q.shape))\n",
    "        \n",
    "        \"\"\"Shape length of 3 indicates an additional dimension for sequence \n",
    "        which requires transposition in split.\n",
    "        \"\"\"\n",
    "        do_transpose = len(dense_q.shape) == 3\n",
    "\n",
    "        split_q = self.split_heads(dense_q, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        split_k = self.split_heads(dense_k, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        split_v = self.split_heads(dense_v, batch_size, do_transpose)  # (batch_size, num_heads, (seq_len,) depth)\n",
    "        \n",
    "        #print(\"split_q.shape == {}\".format(split_q.shape))\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, (seq_len,) depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, )\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            split_q, \n",
    "            split_k, \n",
    "            split_v, \n",
    "            mask=None\n",
    "        )\n",
    "        \n",
    "        #print(\"scaled_attention.shape == {}\".format(scaled_attention.shape))\n",
    "        #print(\"attention_weights.shape == {}\".format(attention_weights.shape))\n",
    "\n",
    "        if do_transpose:\n",
    "            \"\"\"Revert transposition from head split.\n",
    "            \n",
    "            (batch_size, num_heads, seq_len, depth)\n",
    "            ->\n",
    "            (batch_size, seq_length, num_heads, depth)\n",
    "            \"\"\"\n",
    "            scaled_attention = tf.transpose(\n",
    "                scaled_attention,\n",
    "                perm=[0, 2, 1, 3]\n",
    "            ) \n",
    "            #print(\"transposed scaled_attention.shape == {}\".format(scaled_attention.shape))\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (\n",
    "                -1, \n",
    "                self.d_model\n",
    "            )\n",
    "        )  # (batch_size, d_model)\n",
    "        \n",
    "        #print(\"concat_attention.shape == {}\".format(concat_attention.shape))\n",
    "\n",
    "        output = self.dense(\n",
    "            concat_attention\n",
    "        )  # (batch_size, d_model)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model.\"\"\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "class OverlyComplicatedModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = keras.Input(\n",
    "            shape=(input_dim,),\n",
    "        )\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            48,\n",
    "            num_heads=4,\n",
    "        )\n",
    "        \n",
    "        self.normalization_1 = keras.layers.BatchNormalization(\n",
    "            epsilon=1e-6,\n",
    "        )\n",
    "        \n",
    "        self.dense = keras.layers.Dense(\n",
    "            48,\n",
    "            activation=\"relu\",\n",
    "            name=\"dense\",\n",
    "        )\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(\n",
    "            0.5,\n",
    "        )\n",
    "        \n",
    "        self.normalization_2 = keras.layers.BatchNormalization(\n",
    "            epsilon=1e-6,\n",
    "        )\n",
    "        \n",
    "        self.outputs = keras.layers.Dense(\n",
    "            output_dim,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        #x = self.input_layer(inputs)\n",
    "        \n",
    "        x_attn, attention_weights = self.multi_head_attention(inputs)\n",
    "        \n",
    "        x_attn_res = keras.layers.add(\n",
    "            [\n",
    "                inputs,  # Residual connection\n",
    "                x_attn,  \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        x_attn_norm = self.normalization_1(x_attn_res)\n",
    "        \n",
    "        #print(\"x_attn shape: {}\".format(x_attn.shape))\n",
    "        #print(\"Inputs shape: {}\".format(inputs.shape))\n",
    "        \n",
    "        x_dense = self.dense(x_attn_norm)\n",
    "        \n",
    "        x_dense_res = keras.layers.add(\n",
    "            [\n",
    "                x_attn_norm,  # Residual connection\n",
    "                x_dense,  \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #x = self.dropout(\n",
    "        #    x_dense_res, \n",
    "        #    training=training,\n",
    "        #)\n",
    "        \n",
    "        x = self.normalization_2(x_dense_res)\n",
    "        \n",
    "        outputs = self.outputs(x)\n",
    "        \n",
    "        return outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create model.\"\"\"\n",
    "\n",
    "model = OverlyComplicatedModel(\n",
    "    input_dim=x_ar.shape[1],\n",
    "    output_dim=y_ar.shape[1],\n",
    ")\n",
    "\n",
    "\"\"\"The model has NOT yet been built.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the step function as a callable TensorFlow graph.\"\"\"\n",
    "\n",
    "@tf.function()\n",
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"Record operations for automatic differentiation.\"\"\"\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        #print(x_batch.shape)\n",
    "        #print(y_batch.shape)\n",
    "        \n",
    "        predictions, _ = model(x_batch, training=True) \n",
    "\n",
    "        loss = loss_fn(\n",
    "            y_batch, \n",
    "            predictions,\n",
    "        )\n",
    "        \n",
    "    \"\"\"Compute gradients.\"\"\"\n",
    "    gradients = gradient_tape.gradient(loss, model.trainable_weights)\n",
    "        \n",
    "    \"\"\"Compute and apply deltas.\"\"\"\n",
    "    optimizer.apply_gradients(\n",
    "        zip(\n",
    "            gradients, \n",
    "            model.trainable_weights,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    train_loss_metric(loss)\n",
    "    train_acc_metric(y_batch, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model.\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = defaultdict(list)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss_metric.reset_states()\n",
    "    train_acc_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    val_acc_metric.reset_states()\n",
    "\n",
    "    \"\"\"Training\"\"\"\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Epoch %02d | step %04d | %06s samples | loss: %.4f\" % (\n",
    "                    epoch,\n",
    "                    step,\n",
    "                    step * batch_size,\n",
    "                    float(loss)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    history[\"loss\"].append(train_loss_metric.result())\n",
    "    history[\"accuracy\"].append(train_acc_metric.result())\n",
    "\n",
    "    print(\n",
    "        \"Epoch %02d | training accuracy: %.4f\" % (\n",
    "            epoch,\n",
    "            float(train_acc_metric.result())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \"\"\"Validation\"\"\"\n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        predictions, attention_weights = model(x_batch, training=False)\n",
    "        \n",
    "        loss = loss_fn(y_batch, predictions)\n",
    "        \n",
    "        val_loss_metric.update_state(loss)\n",
    "        val_acc_metric.update_state(y_batch, predictions)\n",
    "        \n",
    "        #print(attention_weights.shape)\n",
    "        \n",
    "    history[\"val_loss\"].append(val_loss_metric.result())\n",
    "    history[\"val_accuracy\"].append(val_acc_metric.result())\n",
    "    history[\"attention_weights\"].append(attention_weights)\n",
    "\n",
    "    print(\n",
    "        \"Epoch %02d | validation accuracy: %.4f\" % (\n",
    "            epoch,\n",
    "            float(val_acc_metric.result())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        \"Epoch %02d | time: %.2fs\\n\" % (\n",
    "            epoch,\n",
    "            time.time() - start_time\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f660f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracies.\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    \n",
    "    plt.title('model accuracy')\n",
    "    \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    \n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    plt.savefig('accuracy')    \n",
    "    \n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot losses.\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses(history):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    \n",
    "    plt.title('model loss')\n",
    "    \n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    \n",
    "    plt.legend(['train', 'validation'])\n",
    "    \n",
    "    plt.savefig('loss')\n",
    "    \n",
    "plot_losses(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
